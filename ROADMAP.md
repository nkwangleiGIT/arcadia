## Roadmap of KubeAGI

### v0.1.0 - 2023 Q4 Released

* Dataset Management - manage data, including local files, integrate with object storage(s3), data editing, version control, and file download
* Data Processing - data cleaning, text splitting (e.g., text segmentation, QA splitting), file labeling
* Knowledge Base - data embedding
* Model Management - manage the lifecycle of models.
* Model Serving
  - Support CPU & GPU Model Serving
  - Support both remote and local model inference services, and associate with the knowledge base
  - Support local embedding service (bge, m3e)
  - Support vLLM inference engine
* LLM Applications - prompt engineering, initial implementation of LLM application orchestration capabilities. Manage and orchestrate Prompt, LLM/Retriever Chain nodes, and provide relevant example applications (based on streamlit)
* Guided walkthroughs and example scenarios - let the user get started to build LLM application quickly, add momre built-in chat example applications

### v0.2.0 - 2024 Feb. Ongoing
* Support evaluation of Prompts under different LLMs and generate test reports.
* RAG evaluation and RAG Question Generation
  - Optimize question generation, analyze question quality, filter out low-similarity questions
  - Evaluation metrics: retrieval evaluation - Hit Rate, MRR; answer evaluation - fairness, relevance, consistency, etc
  - Other evaluation capabilities

* Data lineage - Understand the origin and flow of data, e.g., support mapping between answers and original documents
* Perform similarity analysis on QA pairs generated by large models, allowing manual processing (deletion, merging, etc.) by users
* Playground for datasets, knowledge base, model services, etc., based on streamlit.
* LLM applications support Get/Post API Chain, enabling typical LLM application development (non-workflow mode)
* Visualization of various data types, based on streamlit

### v0.3.0 - 2024 Mar.
* Data Processing - Introduce text annotation (automated + manual) to improve data quality through assisted fine-tuning

* Data Security - Support data anonymization (e.g., masking sensitive information like ID numbers, phone numbers, and bank account numbers)

* Enhanced Data Integration - Increase the capability to integrate with various data sources (databases, APIs, etc.) and support data synchronization strategies (automatic synchronization)

* Support manual evaluation to ensure quality control before deploying to production. Additionally, incorporate manual feedback into the monitoring system

* Enable user feedback on the question-answering system to facilitate optimization of LLM applications (data processing, prompt optimization, etc.)

* Integration of GPU management, scheduling, and resource monitoring capabilities for containerized environments

* Integration of API gateway to govern model service APIs, including monitoring, analysis, and security measures, and construct AI gateway


### v0.5.0 - 2024 Apr.
* Support low-resource large model fine-tuning, including RLHF (Reinforcement Learning from Human Feedback), SFT (Semi-Supervised Fine-Tuning) techniques such as Adapter, P-tuning, and LoRA. This improves model quality while reducing performance requirements for model serving (e.g., reducing inference costs, latency issues related to long prompts or slow inference)
* Model compression techniques
* Conduct testing and evaluation of model services and embeddings (QA evaluation, metric collection)

* Implement "scale to zero" capability (integrating with Arbiter) for cold start scenarios, enabling models and applications to evolve towards a Serverless architecture

* Support orchestration of additional node types such as Agent, Cache, etc

* Add more best practices for prompt engineering
  - Few-shot learning techniques
  - Chain-of-Thought (CoT) approach
  - Mind-mapping techniques

### v1.0 - 2024 Jun.
* Automatically constructing prompt templates based on data annotations
* Enhance the monitoring capabilities of LLMOps, monitoring the pipeline from dataset and feature data to model inference, with call chain tracing based on langchain-go
* Implement a pipeline from data source -> dataset -> data processing -> data versioning -> knowledge base -> model service
* Strengthen the Python SDK to handle basic capabilities such as dataset manipulation, data processing, and vectorization. These operations can be performed in a notebook environment.
  - Refer Databricks to enhance the developer experience
* Implement gray release for LLM applications based on AI gateway

